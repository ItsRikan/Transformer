{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fd5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=open('dialogs.txt','r').read().splitlines()\n",
    "text2=pd.read_csv('Conversation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af3fc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2['text'] = text2.question +\" \" + text2.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf4a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2=text2['text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09a5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=text1 + text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6eeccea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7455"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99574199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self,vocab_size=500):\n",
    "        self.vocab_size=vocab_size\n",
    "        self.general_vocab=256\n",
    "        self.sos=\"<|SOS|>\"\n",
    "        self.eos=\"<|EOS|>\"\n",
    "        self.ukn=\"<|UKN|>\"\n",
    "        self.enc_eos=vocab_size-1\n",
    "        self.enc_sos=vocab_size-2\n",
    "        self.enc_ukn=vocab_size-3\n",
    "        self.merges={}\n",
    "        self.vocab={}\n",
    "    \n",
    "    def _to_token(self,text)->list:\n",
    "        tokens=[]\n",
    "        for sen in text:\n",
    "            toks=list(map(int,sen.encode('utf-8')))\n",
    "            toks=[self.enc_sos] + toks + [self.enc_eos]\n",
    "            for t in toks:\n",
    "                tokens.append(t)\n",
    "        return tokens\n",
    "    def _get_stats(self,ids):\n",
    "        frequency={}\n",
    "        for p in zip(ids,ids[1:]):\n",
    "            frequency[p] = frequency.get(p,0) + 1\n",
    "        return frequency\n",
    "\n",
    "    def _merge(self,ids,pair,idx):\n",
    "        new_ids=[]\n",
    "        i=0\n",
    "        while i<len(ids):\n",
    "            if i<len(ids)-1 and ids[i]==pair[0] and ids[i+1] == pair[1]:\n",
    "                new_ids.append(idx)\n",
    "                i+=2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i+=1\n",
    "        return new_ids\n",
    "\n",
    "    def train(self,text):\n",
    "        tokens=self._to_token(text)\n",
    "        for i in range(self.vocab_size - self.general_vocab-1):\n",
    "            idx=self.general_vocab + i\n",
    "            stats=self._get_stats(tokens)\n",
    "            pair=max(stats,key=stats.get)\n",
    "            tokens=self._merge(tokens,pair,idx)\n",
    "            self.merges[pair] = idx\n",
    "        self.vocab={idx : bytes([idx]) for idx in range(self.general_vocab)}\n",
    "        self.vocab[self.enc_sos] = bytes(self.sos.encode('utf-8'))\n",
    "        self.vocab[self.enc_eos] = bytes(self.eos.encode('utf-8'))\n",
    "        self.vocab[self.enc_ukn] = bytes(self.ukn.encode('utf-8'))\n",
    "        for (p0,p1),enc in (self.merges.items()):\n",
    "            self.vocab[enc] = self.vocab[p0] + self.vocab[p1]\n",
    "    def decode(self,tokens):\n",
    "        result = []\n",
    "        for t in tokens:\n",
    "            if t == self.enc_sos:\n",
    "                result.append(self.sos)\n",
    "            elif t == self.enc_eos:\n",
    "                result.append(self.eos)\n",
    "            elif t == self.enc_ukn:\n",
    "                result.append(self.ukn)\n",
    "            elif t in self.vocab:\n",
    "                try:\n",
    "                    result.append(self.vocab[t].decode('utf-8'))\n",
    "                except Exception:\n",
    "                    result.append(self.ukn)\n",
    "            else:\n",
    "                result.append(self.ukn)\n",
    "        return ''.join(result)\n",
    "    def encode(self,text):\n",
    "        tokens=list(map(int,text.encode('utf-8')))\n",
    "        tokens=[self.enc_sos] + tokens + [self.enc_eos]\n",
    "        while len(tokens)>1:\n",
    "            stats=self._get_stats(tokens)\n",
    "            pair=min(stats,key=lambda p:self.merges.get(p,float('inf')))\n",
    "            if pair not in self.merges:\n",
    "                break\n",
    "            idx=self.merges.get(pair,self.enc_ukn)\n",
    "            tokens=self._merge(tokens,pair,idx)\n",
    "        tokens=[t if t in self.vocab else self.enc_ukn for t in tokens]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "616a4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(4000)\n",
    "tokenizer.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f019e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('tokenizer.pkl','wb') as f:\n",
    "    pickle.dump(tokenizer,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b7a6a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pkl','rb') as f:\n",
    "    model=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e252ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=\"\"\"The scientist studied the effects of xenotransplantation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29af8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=model.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c08a979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3998 -> <|SOS|>\n",
      "84 -> T\n",
      "2977 -> he s\n",
      "553 -> ci\n",
      "283 -> en\n",
      "350 -> ti\n",
      "465 -> st \n",
      "895 -> stu\n",
      "305 -> di\n",
      "1342 -> ed the \n",
      "101 -> e\n",
      "1311 -> ffe\n",
      "99 -> c\n",
      "470 -> ts \n",
      "328 -> of \n",
      "120 -> x\n",
      "283 -> en\n",
      "111 -> o\n",
      "1361 -> tr\n",
      "270 -> an\n",
      "115 -> s\n",
      "382 -> pl\n",
      "819 -> ant\n",
      "907 -> ation\n",
      "46 -> .\n",
      "3999 -> <|EOS|>\n"
     ]
    }
   ],
   "source": [
    "for tok in tokens:\n",
    "    print(f\"{tok} -> {model.decode([tok])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
