{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff19a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082a5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8344bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tokenizer import Tokenizer\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81be5947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1=open('dialogs.txt','r').read().splitlines()\n",
    "text2=pd.read_csv('Conversation.csv')\n",
    "text2['text'] = text2.question +\" \" + text2.answer\n",
    "text2=text2['text'].to_list()\n",
    "text=text1 + text2\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6901a886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokens=[]\n",
    "for t in text:\n",
    "    tok=tokenizer.encode(t)\n",
    "    for i in tok:\n",
    "        tokens.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6940411",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=tokenizer.vocab_size\n",
    "block_size=32\n",
    "batch_size=64\n",
    "n_embd=32\n",
    "dropout=0.2\n",
    "n_head=4\n",
    "n_blocks=2\n",
    "learning_rate=3e-4\n",
    "max_iter=3000\n",
    "eval_interval=max_iter//10\n",
    "eval_iters=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02cd1f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_token=115015, 3594, 115009\n"
     ]
    }
   ],
   "source": [
    "n_token=len(tokens)\n",
    "n_packs=(n_token - 1) // block_size\n",
    "n_used= n_packs * block_size + 1\n",
    "print(f\"{n_token=}, {n_packs}, {n_used}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1083d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([115008]) torch.Size([115008])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "X=torch.tensor(tokens[:n_used-1])\n",
    "y=torch.tensor(tokens[1:n_used])\n",
    "print(X.shape,y.shape)\n",
    "X=X.view(-1,block_size)\n",
    "y=y.view(-1,block_size)\n",
    "print(X.shape==y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4492d341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3998,  467, 2778,  492,  333, 1654,  381, 3644,  392, 3645,   63, 3999,\n",
       "         3998,  381, 3644,  392, 3645, 1868, 3647, 3305, 1182,  281,   46, 3999,\n",
       "         3998,  381, 3647, 3305, 1182, 1125, 1661, 1154]),\n",
       " tensor([ 467, 2778,  492,  333, 1654,  381, 3644,  392, 3645,   63, 3999, 3998,\n",
       "          381, 3644,  392, 3645, 1868, 3647, 3305, 1182,  281,   46, 3999, 3998,\n",
       "          381, 3647, 3305, 1182, 1125, 1661, 1154,  392]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0],y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "725c0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=int(len(X) * 0.9)\n",
    "x_train=X[:n]\n",
    "y_train=y[:n]\n",
    "x_test=X[n:]\n",
    "y_test=y[n:]\n",
    "def get_dataset(split):\n",
    "    data_len=len(x_train) if split=='train' else len(x_test)\n",
    "    ixs=torch.randint(0,data_len,(batch_size,))\n",
    "    if split=='train':\n",
    "        X=x_train[ixs]\n",
    "        y=y_train[ixs]\n",
    "    else:\n",
    "        X=x_test[ixs]\n",
    "        y=y_test[ixs]\n",
    "    return X,y\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d208f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHead(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.query=nn.Linear(n_embd,head_size)\n",
    "        self.key=nn.Linear(n_embd,head_size)\n",
    "        self.value=nn.Linear(n_embd,head_size)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,X):\n",
    "        B,T,C=X.shape\n",
    "        q=self.query(X)\n",
    "        k=self.key(X)\n",
    "        head_size=q.shape[-1]\n",
    "        wei=q @ k.transpose(-2,-1) * head_size**-0.5\n",
    "        wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei=F.softmax(wei,dim=-1)\n",
    "        v=self.value(X)\n",
    "        out=wei @ v \n",
    "        out=self.dropout(out)\n",
    "        return out       \n",
    "class MultiHead(nn.Module):\n",
    "    def __init__(self,n_head,head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([SingleHead(head_size) for _ in range(n_head)])\n",
    "        self.norm=nn.LayerNorm(n_embd)\n",
    "        self.proj=nn.Linear(n_embd,n_embd)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.heads],-1)\n",
    "        out=self.norm(out)\n",
    "        out=self.proj(out)\n",
    "        out=self.dropout(out)\n",
    "        return out\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(n_embd,n_embd*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4,n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ma_heads=MultiHead(n_head,n_embd//n_head)\n",
    "        self.feed_forward=FeedForward()\n",
    "        self.l1=nn.LayerNorm(n_embd)\n",
    "        self.l2=nn.LayerNorm(n_embd)\n",
    "    def forward(self,x):\n",
    "        x= x + self.ma_heads(self.l1(x))\n",
    "        x= x + self.feed_forward(self.l2(x))\n",
    "        return x\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self,n_blocks):\n",
    "        super().__init__()\n",
    "        self.token_embd_table=nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embd_table=nn.Embedding(block_size,n_embd)\n",
    "        self.blocks=nn.Sequential(*[Block() for _ in range(n_blocks)])\n",
    "        self.lm_head=nn.Linear(n_embd,vocab_size)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,X,targets=None):\n",
    "        B,T=X.shape\n",
    "        token_embd=self.token_embd_table(X)\n",
    "        positional_embd=self.position_embd_table(torch.arange(T))\n",
    "        x=token_embd + positional_embd\n",
    "        x=self.dropout(x)\n",
    "        x=self.blocks(x)\n",
    "        x=self.dropout(x)\n",
    "        logits=self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B,T,C=logits.shape\n",
    "            logits=logits.view(B*T,C)\n",
    "            targets=targets.view(B*T)\n",
    "            loss=F.cross_entropy(logits,targets)\n",
    "        return logits,loss\n",
    "    def generate(self,X):\n",
    "        while len(X)>0:\n",
    "            if not isinstance(X,torch.Tensor):\n",
    "                X=torch.tensor(X)\n",
    "            X_cond=X[:,-block_size:]\n",
    "            logits,loss=self(X_cond)\n",
    "            logits=logits[:,-1,:]\n",
    "            probs=F.softmax(logits,1)\n",
    "            idx=torch.multinomial(probs,num_samples=1)\n",
    "            if idx.item()==tokenizer.enc_eos:\n",
    "                break\n",
    "            X=torch.cat((X,idx),dim=1)\n",
    "        return X.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d33cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train','test']:\n",
    "        losses=torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            X,y=get_dataset(split)\n",
    "            logits,loss=model(X,y)\n",
    "            losses[i]=loss.item()\n",
    "        out[split]=losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dcb87e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4201879d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0/3000 -> Validation loss : 8.64981746673584 ; Training loss : 8.62234878540039\n",
      "Step 300/3000 -> Validation loss : 6.525638580322266 ; Training loss : 6.309415340423584\n",
      "Step 600/3000 -> Validation loss : 6.364574432373047 ; Training loss : 6.14846658706665\n",
      "Step 900/3000 -> Validation loss : 6.281893730163574 ; Training loss : 6.065360069274902\n",
      "Step 1200/3000 -> Validation loss : 6.232460975646973 ; Training loss : 5.977438926696777\n",
      "Step 1500/3000 -> Validation loss : 6.169061660766602 ; Training loss : 5.872109889984131\n",
      "Step 1800/3000 -> Validation loss : 6.1016998291015625 ; Training loss : 5.7578511238098145\n",
      "Step 2100/3000 -> Validation loss : 6.017408847808838 ; Training loss : 5.6246819496154785\n",
      "Step 2400/3000 -> Validation loss : 5.934736251831055 ; Training loss : 5.466085910797119\n",
      "Step 2700/3000 -> Validation loss : 5.8248419761657715 ; Training loss : 5.306494235992432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(),learning_rate,weight_decay=1e-2)\n",
    "for i in range(max_iter):\n",
    "    if i%eval_interval==0:\n",
    "        loss=estimate_loss()\n",
    "        print(f\"Step {i}/{max_iter} -> Validation loss : {loss['test']} ; Training loss : {loss['train']}\")\n",
    "    xb,yb=get_dataset('train')\n",
    "    logits,loss=model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15d23693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|SOS|>Hello<|EOS|><|SOS|>yes, i my fasomething y you know what your go go to fungame.\n"
     ]
    }
   ],
   "source": [
    "enc_test=tokenizer.encode(\"Hello\")\n",
    "#print(enc_test)\n",
    "gen=model.generate([enc_test])\n",
    "#print(gen)\n",
    "print(tokenizer.decode(gen[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2746c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|SOS|>Hello what's upp!<|EOS|><|SOS|>what do you meancasu?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
